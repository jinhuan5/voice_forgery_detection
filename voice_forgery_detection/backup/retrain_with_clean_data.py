#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä½¿ç”¨æ¸…ç†åçš„æ•°æ®é‡æ–°è®­ç»ƒæ¨¡å‹
"""

import os
import numpy as np
import librosa
import joblib
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix
from sklearn.preprocessing import StandardScaler
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
import glob

class CleanDataRetrainer:
    """ä½¿ç”¨æ¸…ç†åçš„æ•°æ®é‡æ–°è®­ç»ƒæ¨¡å‹"""
    
    def __init__(self):
        """åˆå§‹åŒ–è®­ç»ƒå™¨"""
        self.models = {}
        self.best_model = None
        self.best_score = 0
        self.scaler = None
        self.feature_names = []
    
    def extract_features(self, audio_path):
        """æå–ç‰¹å¾ï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰"""
        try:
            # åŠ è½½éŸ³é¢‘
            audio, sr = librosa.load(audio_path, sr=16000)
            
            features = []
            
            # 1. MFCCç‰¹å¾
            mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13, n_fft=2048, hop_length=512)
            for i in range(mfcc.shape[0]):
                features.extend([
                    float(np.mean(mfcc[i])),      # å‡å€¼
                    float(np.std(mfcc[i])),       # æ ‡å‡†å·®
                    float(np.min(mfcc[i])),       # æœ€å°å€¼
                    float(np.max(mfcc[i]))         # æœ€å¤§å€¼
                ])
            
            # 2. é¢‘è°±ç‰¹å¾
            spectral_centroids = librosa.feature.spectral_centroid(y=audio, sr=sr)
            spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio, sr=sr)
            zcr = librosa.feature.zero_crossing_rate(audio)
            
            features.extend([
                float(np.mean(spectral_centroids)),
                float(np.std(spectral_centroids)),
                float(np.mean(spectral_bandwidth)),
                float(np.std(spectral_bandwidth)),
                float(np.mean(zcr)),
                float(np.std(zcr))
            ])
            
            # 3. èŠ‚å¥ç‰¹å¾
            tempo, _ = librosa.beat.beat_track(y=audio, sr=sr)
            features.append(float(tempo))
            
            # ç¡®ä¿ç‰¹å¾æ•°é‡ä¸º52ä¸ª
            if len(features) > 52:
                features = features[:52]
            elif len(features) < 52:
                features.extend([0.0] * (52 - len(features)))
            
            return np.array(features)
            
        except Exception as e:
            print(f"ç‰¹å¾æå–å¤±è´¥ {audio_path}: {e}")
            return None
    
    def load_clean_data(self, data_dir="data"):
        """åŠ è½½æ¸…ç†åçš„æ•°æ®"""
        print("åŠ è½½æ¸…ç†åçš„æ•°æ®...")
        
        real_features = []
        fake_features = []
        
        # åŠ è½½çœŸå®è¯­éŸ³æ•°æ®
        real_dir = os.path.join(data_dir, "real")
        if os.path.exists(real_dir):
            real_files = glob.glob(os.path.join(real_dir, "*.wav")) + glob.glob(os.path.join(real_dir, "*.mp3"))
            print(f"æ‰¾åˆ° {len(real_files)} ä¸ªçœŸå®è¯­éŸ³æ–‡ä»¶")
            
            for i, file_path in enumerate(real_files):
                print(f"å¤„ç†çœŸå®è¯­éŸ³ {i+1}/{len(real_files)}: {os.path.basename(file_path)}")
                features = self.extract_features(file_path)
                if features is not None:
                    real_features.append(features)
        
        # åŠ è½½ä¼ªé€ è¯­éŸ³æ•°æ®
        fake_dir = os.path.join(data_dir, "fake")
        if os.path.exists(fake_dir):
            fake_files = glob.glob(os.path.join(fake_dir, "*.wav")) + glob.glob(os.path.join(fake_dir, "*.mp3"))
            print(f"æ‰¾åˆ° {len(fake_files)} ä¸ªä¼ªé€ è¯­éŸ³æ–‡ä»¶")
            
            for i, file_path in enumerate(fake_files):
                print(f"å¤„ç†ä¼ªé€ è¯­éŸ³ {i+1}/{len(fake_files)}: {os.path.basename(file_path)}")
                features = self.extract_features(file_path)
                if features is not None:
                    fake_features.append(features)
        
        if len(real_features) == 0 or len(fake_features) == 0:
            print("é”™è¯¯ï¼šæ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„éŸ³é¢‘æ–‡ä»¶")
            return None, None
        
        # åˆå¹¶ç‰¹å¾å’Œæ ‡ç­¾
        print("\nåˆå¹¶ç‰¹å¾å’Œæ ‡ç­¾...")
        
        # åˆ›å»ºæ ‡ç­¾
        real_labels = np.zeros(len(real_features))  # çœŸå®è¯­éŸ³æ ‡ç­¾ä¸º0
        fake_labels = np.ones(len(fake_features))   # ä¼ªé€ è¯­éŸ³æ ‡ç­¾ä¸º1
        
        # åˆå¹¶ç‰¹å¾å’Œæ ‡ç­¾
        X = np.vstack([real_features, fake_features])
        y = np.hstack([real_labels, fake_labels])
        
        print(f"æ•°æ®ç»Ÿè®¡:")
        print(f"  æ€»æ ·æœ¬æ•°: {len(X)}")
        print(f"  çœŸå®è¯­éŸ³: {len(real_features)}")
        print(f"  ä¼ªé€ è¯­éŸ³: {len(fake_features)}")
        print(f"  ç‰¹å¾ç»´åº¦: {X.shape[1]}")
        
        return X, y
    
    def train_models(self, X_train, y_train):
        """è®­ç»ƒæ¨¡å‹"""
        print("å¼€å§‹è®­ç»ƒæ¨¡å‹...")
        
        # å®šä¹‰æ¨¡å‹
        models = {
            'Random Forest': RandomForestClassifier(
                n_estimators=100, 
                max_depth=20, 
                min_samples_split=5,
                random_state=42
            ),
            'Gradient Boosting': GradientBoostingClassifier(
                n_estimators=100,
                learning_rate=0.1,
                max_depth=10,
                random_state=42
            ),
            'SVM (RBF)': SVC(
                kernel='rbf',
                C=1.0,
                gamma='scale',
                probability=True,
                random_state=42
            ),
            'Logistic Regression': LogisticRegression(
                random_state=42,
                max_iter=1000
            )
        }
        
        # è®­ç»ƒæ¯ä¸ªæ¨¡å‹
        for name, model in models.items():
            print(f"è®­ç»ƒ {name}...")
            
            # è®­ç»ƒæ¨¡å‹
            model.fit(X_train, y_train)
            
            # äº¤å‰éªŒè¯è¯„ä¼°
            cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')
            
            # ä¿å­˜æ¨¡å‹å’Œåˆ†æ•°
            self.models[name] = {
                'model': model,
                'cv_mean': cv_scores.mean(),
                'cv_std': cv_scores.std()
            }
            
            print(f"{name} è®­ç»ƒå®Œæˆ:")
            print(f"   äº¤å‰éªŒè¯å‡†ç¡®ç‡: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")
        
        # é€‰æ‹©æœ€ä½³æ¨¡å‹
        self.select_best_model()
    
    def select_best_model(self):
        """é€‰æ‹©æœ€ä½³æ¨¡å‹"""
        print("\né€‰æ‹©æœ€ä½³æ¨¡å‹...")
        
        best_name = None
        best_score = 0
        
        for name, model_info in self.models.items():
            score = model_info['cv_mean']
            if score > best_score:
                best_score = score
                best_name = name
        
        self.best_model = self.models[best_name]['model']
        self.best_score = best_score
        
        print(f"æœ€ä½³æ¨¡å‹: {best_name}")
        print(f"   äº¤å‰éªŒè¯å‡†ç¡®ç‡: {best_score:.4f}")
    
    def evaluate_model(self, X_test, y_test):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        print("\næ¨¡å‹æ€§èƒ½è¯„ä¼°...")
        
        # é¢„æµ‹
        y_pred = self.best_model.predict(X_test)
        y_pred_proba = self.best_model.predict_proba(X_test)[:, 1]
        
        # è®¡ç®—æŒ‡æ ‡
        accuracy = (y_pred == y_test).mean()
        auc_score = roc_auc_score(y_test, y_pred_proba)
        
        print(f"æµ‹è¯•é›†æ€§èƒ½:")
        print(f"   å‡†ç¡®ç‡: {accuracy:.4f}")
        print(f"   AUC: {auc_score:.4f}")
        
        # è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
        print("\nè¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(y_test, y_pred, target_names=['Real', 'Fake']))
        
        return accuracy, auc_score, y_pred, y_pred_proba
    
    def plot_confusion_matrix(self, y_test, y_pred, save_path="clean_confusion_matrix.png"):
        """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
        try:
            cm = confusion_matrix(y_test, y_pred)
            
            plt.figure(figsize=(8, 6))
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                       xticklabels=['Real', 'Fake'], 
                       yticklabels=['Real', 'Fake'])
            plt.title('Clean Data Model Confusion Matrix')
            plt.xlabel('Predicted')
            plt.ylabel('Actual')
            plt.tight_layout()
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            plt.close()
            print(f"æ··æ·†çŸ©é˜µä¿å­˜æˆåŠŸ: {save_path}")
        except Exception as e:
            print(f"ä¿å­˜æ··æ·†çŸ©é˜µå¤±è´¥: {e}")
    
    def plot_roc_curve(self, y_test, y_pred_proba, save_path="clean_roc_curve.png"):
        """ç»˜åˆ¶ROCæ›²çº¿"""
        try:
            from sklearn.metrics import roc_curve, auc
            
            fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
            roc_auc = auc(fpr, tpr)
            
            plt.figure(figsize=(8, 6))
            plt.plot(fpr, tpr, color='darkorange', lw=2, 
                    label=f'ROC curve (AUC = {roc_auc:.2f})')
            plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
            plt.xlim([0.0, 1.0])
            plt.ylim([0.0, 1.05])
            plt.xlabel('False Positive Rate')
            plt.ylabel('True Positive Rate')
            plt.title('Clean Data Model ROC Curve')
            plt.legend(loc="lower right")
            plt.grid(True)
            plt.tight_layout()
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            plt.close()
            print(f"ROCæ›²çº¿ä¿å­˜æˆåŠŸ: {save_path}")
        except Exception as e:
            print(f"ä¿å­˜ROCæ›²çº¿å¤±è´¥: {e}")
    
    def save_model(self, model_dir="models"):
        """ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹"""
        print("ä¿å­˜æ¸…ç†åçš„æ¨¡å‹...")
        
        try:
            # åˆ›å»ºæ¨¡å‹ç›®å½•
            os.makedirs(model_dir, exist_ok=True)
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            model_path = os.path.join(model_dir, "clean_detector.pkl")
            joblib.dump(self.best_model, model_path)
            print(f"æ¸…ç†æ¨¡å‹ä¿å­˜æˆåŠŸ: {model_path}")
            
            # ä¿å­˜æ ‡å‡†åŒ–å™¨
            scaler_path = os.path.join(model_dir, "clean_scaler.pkl")
            joblib.dump(self.scaler, scaler_path)
            print(f"æ¸…ç†æ ‡å‡†åŒ–å™¨ä¿å­˜æˆåŠŸ: {scaler_path}")
            
            # ä¿å­˜æ¨¡å‹ä¿¡æ¯
            model_info = {
                'model_name': type(self.best_model).__name__,
                'best_score': self.best_score,
                'training_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'feature_count': 'Clean data features'
            }
            
            info_path = os.path.join(model_dir, "clean_model_info.txt")
            with open(info_path, 'w', encoding='utf-8') as f:
                for key, value in model_info.items():
                    f.write(f"{key}: {value}\n")
            
            print(f"æ¸…ç†æ¨¡å‹ä¿¡æ¯ä¿å­˜æˆåŠŸ: {info_path}")
            return True
            
        except Exception as e:
            print(f"ä¿å­˜æ¨¡å‹æ—¶å‡ºé”™: {e}")
            return False
    
    def train_complete_pipeline(self):
        """å®Œæ•´çš„è®­ç»ƒæµç¨‹"""
        print("å¼€å§‹ä½¿ç”¨æ¸…ç†åçš„æ•°æ®é‡æ–°è®­ç»ƒæ¨¡å‹")
        print("=" * 60)
        
        # 1. åŠ è½½æ¸…ç†åçš„æ•°æ®
        X, y = self.load_clean_data()
        if X is None or y is None:
            print("æ•°æ®åŠ è½½å¤±è´¥")
            return False
        
        # 2. ç‰¹å¾æ ‡å‡†åŒ–
        print("\nç‰¹å¾æ ‡å‡†åŒ–...")
        self.scaler = StandardScaler()
        X_scaled = self.scaler.fit_transform(X)
        
        # 3. åˆ’åˆ†æ•°æ®
        print("åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†...")
        X_train, X_test, y_train, y_test = train_test_split(
            X_scaled, y, test_size=0.2, random_state=42, stratify=y
        )
        print(f"æ•°æ®åˆ’åˆ†å®Œæˆ:")
        print(f"   è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
        print(f"   æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬")
        
        # 4. è®­ç»ƒæ¨¡å‹
        self.train_models(X_train, y_train)
        
        # 5. è¯„ä¼°æ¨¡å‹
        accuracy, auc_score, y_pred, y_pred_proba = self.evaluate_model(X_test, y_test)
        
        # 6. ç»˜åˆ¶å›¾è¡¨
        self.plot_confusion_matrix(y_test, y_pred)
        self.plot_roc_curve(y_test, y_pred_proba)
        
        # 7. ä¿å­˜æ¨¡å‹
        if self.save_model():
            print("\næ¸…ç†åçš„æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
            print(f"æœ€ç»ˆæ€§èƒ½: å‡†ç¡®ç‡ {accuracy:.4f}, AUC {auc_score:.4f}")
            return True
        else:
            print("\næ¸…ç†åçš„æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œä½†ä¿å­˜å¤±è´¥")
            return False

def main():
    """ä¸»å‡½æ•°"""
    # åˆ›å»ºæ¸…ç†æ•°æ®è®­ç»ƒå™¨
    trainer = CleanDataRetrainer()
    
    # è¿è¡Œå®Œæ•´è®­ç»ƒæµç¨‹
    success = trainer.train_complete_pipeline()
    
    if success:
        print("\nğŸ‰ ä½¿ç”¨æ¸…ç†åçš„æ•°æ®é‡æ–°è®­ç»ƒå®Œæˆï¼")
        print("ç°åœ¨å¯ä»¥ä½¿ç”¨æ›´å‡†ç¡®çš„æ¨¡å‹è¿›è¡Œæ£€æµ‹äº†")
    else:
        print("\nâŒ é‡æ–°è®­ç»ƒå¤±è´¥")

if __name__ == "__main__":
    main()
